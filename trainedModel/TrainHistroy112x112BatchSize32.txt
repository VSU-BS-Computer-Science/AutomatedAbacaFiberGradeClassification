Using TensorFlow backend.
Loading dataset...
['training_set(ClassifyBySirArielCinco)\\Current(I)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\4.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\9.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\4.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\9.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\4.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\9.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\4.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\9.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\4.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\9.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\4.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\9.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\4.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\9.jpg']
['training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\9.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\4.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\9.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\4.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\9.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\4.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\9.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\9.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\4.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\1.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\6.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\9.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\4.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\9.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky two(S2)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\2.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\4.jpg', 'training_set(ClassifyBySirArielCinco)\\Streaky three(S3)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft second(G)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\7.jpg', 'training_set(ClassifyBySirArielCinco)\\Seconds(JK)\\8.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\10.jpg', 'training_set(ClassifyBySirArielCinco)\\Soft brown(H)\\5.jpg', 'training_set(ClassifyBySirArielCinco)\\Current(I)\\3.jpg', 'training_set(ClassifyBySirArielCinco)\\Medium brown(M1)\\4.jpg']
[[[[0.78823529 0.78039216 0.77647059]
   [0.44705882 0.39215686 0.40784314]
   [0.8745098  0.87843137 0.84313725]
   ...
   [0.85098039 0.83137255 0.81960784]
   [0.78823529 0.77647059 0.76078431]
   [0.63137255 0.62745098 0.61960784]]

  [[0.42745098 0.39607843 0.40784314]
   [0.45882353 0.41176471 0.43137255]
   [0.85098039 0.85882353 0.83529412]
   ...
   [0.81176471 0.8        0.78823529]
   [0.70980392 0.71764706 0.70980392]
   [0.59607843 0.58823529 0.58431373]]

  [[0.55294118 0.57647059 0.59607843]
   [0.60784314 0.57647059 0.58431373]
   [0.76078431 0.78431373 0.76470588]
   ...
   [0.79215686 0.77647059 0.77254902]
   [0.48627451 0.52941176 0.52941176]
   [0.54901961 0.54509804 0.52941176]]

  ...

  [[0.69019608 0.69019608 0.69019608]
   [0.63529412 0.65098039 0.65098039]
   [0.72156863 0.73333333 0.7254902 ]
   ...
   [0.56862745 0.6        0.59607843]
   [0.6627451  0.68627451 0.67843137]
   [0.55294118 0.59607843 0.61176471]]

  [[0.45882353 0.43529412 0.43921569]
   [0.54117647 0.56078431 0.56470588]
   [0.75294118 0.76862745 0.74901961]
   ...
   [0.56862745 0.6        0.59607843]
   [0.70196078 0.7254902  0.71372549]
   [0.49411765 0.54117647 0.57647059]]

  [[0.28235294 0.24705882 0.23529412]
   [0.61568627 0.62745098 0.64313725]
   [0.70588235 0.72156863 0.70196078]
   ...
   [0.58039216 0.61176471 0.61176471]
   [0.76078431 0.78039216 0.77647059]
   [0.4745098  0.53333333 0.55686275]]]


 [[[0.64705882 0.67058824 0.71372549]
   [0.64313725 0.6745098  0.70980392]
   [0.70196078 0.7254902  0.75294118]
   ...
   [0.68627451 0.65098039 0.6627451 ]
   [0.77254902 0.7254902  0.71764706]
   [0.83921569 0.80392157 0.79215686]]

  [[0.64313725 0.67058824 0.70980392]
   [0.6745098  0.70196078 0.74117647]
   [0.70588235 0.72156863 0.74117647]
   ...
   [0.88235294 0.83921569 0.82352941]
   [0.78431373 0.74117647 0.7254902 ]
   [0.91764706 0.87058824 0.84705882]]

  [[0.59215686 0.62352941 0.6745098 ]
   [0.67843137 0.69411765 0.7254902 ]
   [0.74509804 0.75686275 0.77254902]
   ...
   [0.94509804 0.91372549 0.90588235]
   [0.84705882 0.78823529 0.77254902]
   [0.90588235 0.85882353 0.83529412]]

  ...

  [[0.55294118 0.63529412 0.69411765]
   [0.8        0.80784314 0.81176471]
   [0.48235294 0.54509804 0.60784314]
   ...
   [0.80392157 0.81960784 0.82352941]
   [0.78039216 0.76078431 0.7372549 ]
   [0.81960784 0.83137255 0.83137255]]

  [[0.53333333 0.59607843 0.65098039]
   [0.74901961 0.76862745 0.77647059]
   [0.58039216 0.60784314 0.65490196]
   ...
   [0.76470588 0.78039216 0.78431373]
   [0.81568627 0.78823529 0.77647059]
   [0.74117647 0.74509804 0.74509804]]

  [[0.50196078 0.55686275 0.62745098]
   [0.75294118 0.77254902 0.78039216]
   [0.74901961 0.76078431 0.79215686]
   ...
   [0.70588235 0.7254902  0.72941176]
   [0.81176471 0.78431373 0.77254902]
   [0.73333333 0.7372549  0.74509804]]]


 [[[0.58431373 0.58039216 0.61568627]
   [0.65882353 0.67843137 0.69019608]
   [0.8        0.80784314 0.80784314]
   ...
   [0.78039216 0.76862745 0.76470588]
   [0.71764706 0.70980392 0.70980392]
   [0.61568627 0.61176471 0.62745098]]

  [[0.55686275 0.56470588 0.6       ]
   [0.55294118 0.56078431 0.57647059]
   [0.85882353 0.85882353 0.85882353]
   ...
   [0.77647059 0.75686275 0.75294118]
   [0.71372549 0.71372549 0.71372549]
   [0.71764706 0.69411765 0.69411765]]

  [[0.68627451 0.69019608 0.70588235]
   [0.70196078 0.71764706 0.73333333]
   [0.5254902  0.54509804 0.55686275]
   ...
   [0.78431373 0.76470588 0.76078431]
   [0.70588235 0.70196078 0.70980392]
   [0.6627451  0.63529412 0.63529412]]

  ...

  [[0.84705882 0.83529412 0.81568627]
   [0.66666667 0.69019608 0.70196078]
   [0.73333333 0.7254902  0.72156863]
   ...
   [0.91372549 0.8745098  0.84705882]
   [0.7254902  0.69411765 0.6627451 ]
   [0.80784314 0.80392157 0.78823529]]

  [[0.78039216 0.77254902 0.74117647]
   [0.73333333 0.74509804 0.74901961]
   [0.58431373 0.56470588 0.56862745]
   ...
   [0.70980392 0.68627451 0.65882353]
   [0.62352941 0.63137255 0.62352941]
   [0.78039216 0.77254902 0.75294118]]

  [[0.61176471 0.59215686 0.56078431]
   [0.61176471 0.63137255 0.64705882]
   [0.36078431 0.4        0.42745098]
   ...
   [0.87058824 0.84705882 0.80392157]
   [0.63921569 0.68235294 0.69803922]
   [0.78039216 0.75686275 0.72941176]]]


 ...


 [[[0.6627451  0.69411765 0.69411765]
   [0.36862745 0.34117647 0.36862745]
   [0.67843137 0.70588235 0.69411765]
   ...
   [0.50980392 0.47843137 0.48235294]
   [0.59215686 0.55294118 0.57647059]
   [0.67843137 0.67058824 0.69411765]]

  [[0.62745098 0.65490196 0.65490196]
   [0.39215686 0.35294118 0.36078431]
   [0.67843137 0.70588235 0.70196078]
   ...
   [0.56470588 0.54117647 0.56078431]
   [0.61960784 0.58431373 0.59607843]
   [0.49803922 0.4627451  0.50196078]]

  [[0.55294118 0.55686275 0.55686275]
   [0.37647059 0.3372549  0.3372549 ]
   [0.67058824 0.70196078 0.70196078]
   ...
   [0.71372549 0.71764706 0.73333333]
   [0.54901961 0.49803922 0.49803922]
   [0.52941176 0.50196078 0.52941176]]

  ...

  [[0.30196078 0.29019608 0.29803922]
   [0.36862745 0.3372549  0.3372549 ]
   [0.49411765 0.46666667 0.47843137]
   ...
   [0.47058824 0.46666667 0.47843137]
   [0.56470588 0.59215686 0.60392157]
   [0.51764706 0.52156863 0.5372549 ]]

  [[0.30196078 0.28235294 0.28627451]
   [0.36078431 0.3254902  0.31764706]
   [0.49411765 0.4745098  0.48235294]
   ...
   [0.47058824 0.4745098  0.48627451]
   [0.78039216 0.80784314 0.81960784]
   [0.48627451 0.47843137 0.50196078]]

  [[0.40392157 0.40392157 0.40392157]
   [0.25882353 0.22352941 0.21176471]
   [0.54117647 0.50980392 0.50980392]
   ...
   [0.4627451  0.47058824 0.4745098 ]
   [0.68235294 0.70980392 0.72156863]
   [0.54901961 0.54117647 0.56470588]]]


 [[[0.78431373 0.80392157 0.8       ]
   [0.73333333 0.76470588 0.78431373]
   [0.71372549 0.73333333 0.7372549 ]
   ...
   [0.76078431 0.74901961 0.72941176]
   [0.61568627 0.62745098 0.67058824]
   [0.81568627 0.72941176 0.70196078]]

  [[0.70980392 0.71764706 0.72156863]
   [0.74509804 0.76862745 0.78431373]
   [0.69411765 0.70980392 0.71372549]
   ...
   [0.78039216 0.77647059 0.76078431]
   [0.65098039 0.6627451  0.70588235]
   [0.80392157 0.72156863 0.69411765]]

  [[0.61960784 0.63137255 0.63137255]
   [0.67843137 0.70588235 0.71764706]
   [0.7254902  0.74117647 0.74117647]
   ...
   [0.75294118 0.74117647 0.72156863]
   [0.67843137 0.6745098  0.71372549]
   [0.77254902 0.69803922 0.66666667]]

  ...

  [[0.77254902 0.78039216 0.76470588]
   [0.49803922 0.50980392 0.5254902 ]
   [0.66666667 0.66666667 0.66666667]
   ...
   [0.70196078 0.7372549  0.74901961]
   [0.69411765 0.70196078 0.70196078]
   [0.75294118 0.74117647 0.70980392]]

  [[0.78039216 0.79215686 0.77254902]
   [0.50196078 0.51764706 0.5254902 ]
   [0.7254902  0.73333333 0.7372549 ]
   ...
   [0.71764706 0.74901961 0.74901961]
   [0.76078431 0.77647059 0.78039216]
   [0.67843137 0.69803922 0.70196078]]

  [[0.85882353 0.8627451  0.84705882]
   [0.6627451  0.68235294 0.67058824]
   [0.58431373 0.61176471 0.62745098]
   ...
   [0.69803922 0.7254902  0.73333333]
   [0.76078431 0.78039216 0.78431373]
   [0.62745098 0.65882353 0.68627451]]]


 [[[0.74901961 0.76862745 0.78039216]
   [0.57254902 0.61568627 0.64705882]
   [0.35294118 0.35686275 0.38823529]
   ...
   [0.5254902  0.50588235 0.54117647]
   [0.48235294 0.50196078 0.51764706]
   [0.83137255 0.81960784 0.78823529]]

  [[0.74117647 0.76862745 0.78039216]
   [0.58823529 0.63137255 0.6627451 ]
   [0.41960784 0.44313725 0.48235294]
   ...
   [0.54117647 0.5254902  0.54901961]
   [0.61960784 0.61568627 0.62352941]
   [0.83137255 0.81960784 0.78823529]]

  [[0.69803922 0.70980392 0.71372549]
   [0.60784314 0.65490196 0.68235294]
   [0.61568627 0.65490196 0.68235294]
   ...
   [0.55686275 0.54117647 0.56470588]
   [0.63137255 0.62745098 0.62745098]
   [0.81960784 0.80784314 0.77254902]]

  ...

  [[0.36862745 0.34901961 0.36862745]
   [0.74509804 0.76078431 0.7372549 ]
   [0.4        0.41568627 0.43529412]
   ...
   [0.51764706 0.48627451 0.48627451]
   [0.56078431 0.56862745 0.56470588]
   [0.58823529 0.63529412 0.65098039]]

  [[0.42745098 0.41568627 0.42352941]
   [0.74117647 0.75294118 0.74509804]
   [0.41960784 0.43529412 0.45490196]
   ...
   [0.49019608 0.4745098  0.46666667]
   [0.51764706 0.53333333 0.53333333]
   [0.59607843 0.64705882 0.65882353]]

  [[0.49411765 0.49411765 0.51764706]
   [0.74117647 0.75294118 0.74509804]
   [0.4627451  0.48627451 0.50588235]
   ...
   [0.50196078 0.48235294 0.48627451]
   [0.38039216 0.4        0.4       ]
   [0.62745098 0.67058824 0.68627451]]]]
Compiling model...
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_6 (Conv2D)            (None, 112, 112, 32)      896       
_________________________________________________________________
activation_8 (Activation)    (None, 112, 112, 32)      0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 112, 112, 32)      128       
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 37, 37, 32)        0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 37, 37, 32)        0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 37, 37, 64)        18496     
_________________________________________________________________
activation_9 (Activation)    (None, 37, 37, 64)        0         
_________________________________________________________________
batch_normalization_8 (Batch (None, 37, 37, 64)        256       
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 37, 37, 64)        36928     
_________________________________________________________________
activation_10 (Activation)   (None, 37, 37, 64)        0         
_________________________________________________________________
batch_normalization_9 (Batch (None, 37, 37, 64)        256       
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 18, 18, 64)        0         
_________________________________________________________________
dropout_5 (Dropout)          (None, 18, 18, 64)        0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 18, 18, 128)       73856     
_________________________________________________________________
activation_11 (Activation)   (None, 18, 18, 128)       0         
_________________________________________________________________
batch_normalization_10 (Batc (None, 18, 18, 128)       512       
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 18, 18, 128)       147584    
_________________________________________________________________
activation_12 (Activation)   (None, 18, 18, 128)       0         
_________________________________________________________________
batch_normalization_11 (Batc (None, 18, 18, 128)       512       
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 9, 9, 128)         0         
_________________________________________________________________
dropout_6 (Dropout)          (None, 9, 9, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 10368)             0         
_________________________________________________________________
dense_3 (Dense)              (None, 1024)              10617856  
_________________________________________________________________
activation_13 (Activation)   (None, 1024)              0         
_________________________________________________________________
batch_normalization_12 (Batc (None, 1024)              4096      
_________________________________________________________________
dense_4 (Dense)              (None, 7)                 7175      
_________________________________________________________________
activation_14 (Activation)   (None, 7)                 0         
=================================================================
Total params: 10,908,551
Trainable params: 10,905,671
Non-trainable params: 2,880
_________________________________________________________________
Training network...
Epoch 1/100
2/2 [==============================] - 19s 10s/step - loss: 2.2707 - acc: 0.2344

Epoch 00001: loss improved from inf to 2.27066, saving model to abacaFiberBestModel.model
Epoch 2/100
2/2 [==============================] - 9s 5s/step - loss: 2.1377 - acc: 0.3973

Epoch 00002: loss improved from 2.27066 to 2.09075, saving model to abacaFiberBestModel.model
Epoch 3/100
2/2 [==============================] - 9s 4s/step - loss: 1.3950 - acc: 0.4932

Epoch 00003: loss improved from 2.09075 to 1.35288, saving model to abacaFiberBestModel.model
Epoch 4/100
2/2 [==============================] - 14s 7s/step - loss: 1.0259 - acc: 0.6406

Epoch 00004: loss improved from 1.35288 to 1.02586, saving model to abacaFiberBestModel.model
Epoch 5/100
2/2 [==============================] - 9s 4s/step - loss: 1.0400 - acc: 0.6727

Epoch 00005: loss improved from 1.02586 to 1.01048, saving model to abacaFiberBestModel.model
Epoch 6/100
2/2 [==============================] - 9s 4s/step - loss: 1.6690 - acc: 0.5508

Epoch 00006: loss did not improve from 1.01048
Epoch 7/100
2/2 [==============================] - 14s 7s/step - loss: 1.1876 - acc: 0.6250

Epoch 00007: loss did not improve from 1.01048
Epoch 8/100
2/2 [==============================] - 9s 4s/step - loss: 1.2419 - acc: 0.6219

Epoch 00008: loss did not improve from 1.01048
Epoch 9/100
2/2 [==============================] - 9s 4s/step - loss: 1.0408 - acc: 0.6411

Epoch 00009: loss improved from 1.01048 to 0.96811, saving model to abacaFiberBestModel.model
Epoch 10/100
2/2 [==============================] - 14s 7s/step - loss: 0.7759 - acc: 0.7812

Epoch 00010: loss improved from 0.96811 to 0.77588, saving model to abacaFiberBestModel.model
Epoch 11/100
2/2 [==============================] - 9s 4s/step - loss: 1.1121 - acc: 0.6411

Epoch 00011: loss did not improve from 0.77588
Epoch 12/100
2/2 [==============================] - 9s 4s/step - loss: 0.5790 - acc: 0.8465

Epoch 00012: loss improved from 0.77588 to 0.72730, saving model to abacaFiberBestModel.model
Epoch 13/100
2/2 [==============================] - 14s 7s/step - loss: 0.7633 - acc: 0.7812

Epoch 00013: loss did not improve from 0.72730
Epoch 14/100
2/2 [==============================] - 9s 4s/step - loss: 1.0169 - acc: 0.5768

Epoch 00014: loss did not improve from 0.72730
Epoch 15/100
2/2 [==============================] - 9s 4s/step - loss: 0.6595 - acc: 0.7889

Epoch 00015: loss did not improve from 0.72730
Epoch 16/100
2/2 [==============================] - 14s 7s/step - loss: 0.5563 - acc: 0.7812

Epoch 00016: loss improved from 0.72730 to 0.55634, saving model to abacaFiberBestModel.model
Epoch 17/100
2/2 [==============================] - 9s 4s/step - loss: 0.5837 - acc: 0.8781

Epoch 00017: loss improved from 0.55634 to 0.41544, saving model to abacaFiberBestModel.model
Epoch 18/100
2/2 [==============================] - 9s 4s/step - loss: 0.5716 - acc: 0.8205

Epoch 00018: loss did not improve from 0.41544
Epoch 19/100
2/2 [==============================] - 14s 7s/step - loss: 0.8580 - acc: 0.7500

Epoch 00019: loss did not improve from 0.41544
Epoch 20/100
2/2 [==============================] - 9s 4s/step - loss: 0.6353 - acc: 0.7370

Epoch 00020: loss did not improve from 0.41544
Epoch 21/100
2/2 [==============================] - 9s 4s/step - loss: 1.1317 - acc: 0.6027

Epoch 00021: loss did not improve from 0.41544
Epoch 22/100
2/2 [==============================] - 14s 7s/step - loss: 0.6379 - acc: 0.8125

Epoch 00022: loss did not improve from 0.41544
Epoch 23/100
2/2 [==============================] - 9s 4s/step - loss: 0.5705 - acc: 0.7370

Epoch 00023: loss did not improve from 0.41544
Epoch 24/100
2/2 [==============================] - 9s 4s/step - loss: 0.4308 - acc: 0.8205

Epoch 00024: loss improved from 0.41544 to 0.39814, saving model to abacaFiberBestModel.model
Epoch 25/100
2/2 [==============================] - 14s 7s/step - loss: 0.3064 - acc: 0.9062

Epoch 00025: loss improved from 0.39814 to 0.30642, saving model to abacaFiberBestModel.model
Epoch 26/100
2/2 [==============================] - 9s 4s/step - loss: 0.6465 - acc: 0.7686

Epoch 00026: loss did not improve from 0.30642
Epoch 27/100
2/2 [==============================] - 9s 4s/step - loss: 0.4512 - acc: 0.7946

Epoch 00027: loss did not improve from 0.30642
Epoch 28/100
2/2 [==============================] - 15s 7s/step - loss: 0.4132 - acc: 0.8750

Epoch 00028: loss did not improve from 0.30642
Epoch 29/100
2/2 [==============================] - 9s 4s/step - loss: 1.4244 - acc: 0.5824

Epoch 00029: loss did not improve from 0.30642
Epoch 30/100
2/2 [==============================] - 9s 4s/step - loss: 0.5734 - acc: 0.7754

Epoch 00030: loss did not improve from 0.30642
Epoch 31/100
2/2 [==============================] - 14s 7s/step - loss: 0.2479 - acc: 0.9375

Epoch 00031: loss improved from 0.30642 to 0.24788, saving model to abacaFiberBestModel.model
Epoch 32/100
2/2 [==============================] - 9s 4s/step - loss: 0.6096 - acc: 0.8205

Epoch 00032: loss did not improve from 0.24788
Epoch 33/100
2/2 [==============================] - 9s 4s/step - loss: 0.3169 - acc: 0.8589

Epoch 00033: loss did not improve from 0.24788
Epoch 34/100
2/2 [==============================] - 15s 7s/step - loss: 0.2626 - acc: 0.9219

Epoch 00034: loss did not improve from 0.24788
Epoch 35/100
2/2 [==============================] - 9s 5s/step - loss: 1.2461 - acc: 0.5632

Epoch 00035: loss did not improve from 0.24788
Epoch 36/100
2/2 [==============================] - 9s 5s/step - loss: 0.4690 - acc: 0.8781

Epoch 00036: loss did not improve from 0.24788
Epoch 37/100
2/2 [==============================] - 14s 7s/step - loss: 0.3658 - acc: 0.8281

Epoch 00037: loss did not improve from 0.24788
Epoch 38/100
2/2 [==============================] - 9s 5s/step - loss: 0.6864 - acc: 0.7686

Epoch 00038: loss did not improve from 0.24788
Epoch 39/100
2/2 [==============================] - 9s 4s/step - loss: 0.5174 - acc: 0.9165

Epoch 00039: loss did not improve from 0.24788
Epoch 40/100
2/2 [==============================] - 15s 7s/step - loss: 0.4182 - acc: 0.8281

Epoch 00040: loss did not improve from 0.24788
Epoch 41/100
2/2 [==============================] - 9s 4s/step - loss: 0.4237 - acc: 0.8973

Epoch 00041: loss did not improve from 0.24788
Epoch 42/100
2/2 [==============================] - 9s 4s/step - loss: 0.3654 - acc: 0.9041

Epoch 00042: loss did not improve from 0.24788
Epoch 43/100
2/2 [==============================] - 14s 7s/step - loss: 0.3149 - acc: 0.8594

Epoch 00043: loss did not improve from 0.24788
Epoch 44/100
2/2 [==============================] - 8s 4s/step - loss: 0.4314 - acc: 0.8205

Epoch 00044: loss did not improve from 0.24788
Epoch 45/100
2/2 [==============================] - 9s 4s/step - loss: 0.3805 - acc: 0.8849

Epoch 00045: loss did not improve from 0.24788
Epoch 46/100
2/2 [==============================] - 15s 7s/step - loss: 0.2914 - acc: 0.8438

Epoch 00046: loss did not improve from 0.24788
Epoch 47/100
2/2 [==============================] - 9s 4s/step - loss: 0.2685 - acc: 0.9232

Epoch 00047: loss did not improve from 0.24788
Epoch 48/100
2/2 [==============================] - 9s 4s/step - loss: 0.2447 - acc: 0.9357

Epoch 00048: loss improved from 0.24788 to 0.14821, saving model to abacaFiberBestModel.model
Epoch 49/100
2/2 [==============================] - 14s 7s/step - loss: 0.1527 - acc: 0.9375

Epoch 00049: loss did not improve from 0.14821
Epoch 50/100
2/2 [==============================] - 9s 4s/step - loss: 0.9927 - acc: 0.7178

Epoch 00050: loss did not improve from 0.14821
Epoch 51/100
2/2 [==============================] - 9s 4s/step - loss: 0.4633 - acc: 0.7495

Epoch 00051: loss did not improve from 0.14821
Epoch 52/100
2/2 [==============================] - 15s 7s/step - loss: 0.1575 - acc: 0.9062

Epoch 00052: loss did not improve from 0.14821
Epoch 53/100
2/2 [==============================] - 9s 4s/step - loss: 0.2190 - acc: 0.9232

Epoch 00053: loss did not improve from 0.14821
Epoch 54/100
2/2 [==============================] - 9s 4s/step - loss: 0.3773 - acc: 0.8330

Epoch 00054: loss did not improve from 0.14821
Epoch 55/100
2/2 [==============================] - 14s 7s/step - loss: 0.4246 - acc: 0.8281

Epoch 00055: loss did not improve from 0.14821
Epoch 56/100
2/2 [==============================] - 9s 4s/step - loss: 0.4286 - acc: 0.7946

Epoch 00056: loss did not improve from 0.14821
Epoch 57/100
2/2 [==============================] - 9s 4s/step - loss: 0.0810 - acc: 1.0000

Epoch 00057: loss improved from 0.14821 to 0.07495, saving model to abacaFiberBestModel.model - best model in a training parameters
Epoch 58/100
2/2 [==============================] - 15s 7s/step - loss: 0.1625 - acc: 0.9531

Epoch 00058: loss did not improve from 0.07495
Epoch 59/100
2/2 [==============================] - 9s 4s/step - loss: 0.1786 - acc: 0.9165

Epoch 00059: loss did not improve from 0.07495
Epoch 60/100
2/2 [==============================] - 9s 5s/step - loss: 0.5140 - acc: 0.8138

Epoch 00060: loss did not improve from 0.07495
Epoch 61/100
2/2 [==============================] - 15s 8s/step - loss: 0.2786 - acc: 0.8906

Epoch 00061: loss did not improve from 0.07495
Epoch 62/100
2/2 [==============================] - 10s 5s/step - loss: 1.1069 - acc: 0.7495

Epoch 00062: loss did not improve from 0.07495
Epoch 63/100
2/2 [==============================] - 10s 5s/step - loss: 0.2652 - acc: 0.8781

Epoch 00063: loss did not improve from 0.07495
Epoch 64/100
2/2 [==============================] - 15s 8s/step - loss: 0.2745 - acc: 0.9375

Epoch 00064: loss did not improve from 0.07495
Epoch 65/100
2/2 [==============================] - 9s 4s/step - loss: 1.2543 - acc: 0.7427

Epoch 00065: loss did not improve from 0.07495
Epoch 66/100
2/2 [==============================] - 9s 4s/step - loss: 0.2835 - acc: 0.8781

Epoch 00066: loss did not improve from 0.07495
Epoch 67/100
2/2 [==============================] - 14s 7s/step - loss: 0.1661 - acc: 0.9219

Epoch 00067: loss did not improve from 0.07495
Epoch 68/100
2/2 [==============================] - 9s 4s/step - loss: 0.1513 - acc: 0.9808

Epoch 00068: loss did not improve from 0.07495
Epoch 69/100
2/2 [==============================] - 9s 4s/step - loss: 0.3361 - acc: 0.8713

Epoch 00069: loss did not improve from 0.07495
Epoch 70/100
2/2 [==============================] - 14s 7s/step - loss: 0.1840 - acc: 0.9531

Epoch 00070: loss did not improve from 0.07495
Epoch 71/100
2/2 [==============================] - 9s 4s/step - loss: 0.1648 - acc: 0.9165

Epoch 00071: loss did not improve from 0.07495
Epoch 72/100
2/2 [==============================] - 9s 4s/step - loss: 0.7465 - acc: 0.7303

Epoch 00072: loss did not improve from 0.07495
Epoch 73/100
2/2 [==============================] - 14s 7s/step - loss: 0.1957 - acc: 0.9219

Epoch 00073: loss did not improve from 0.07495
Epoch 74/100
2/2 [==============================] - 9s 4s/step - loss: 1.4597 - acc: 0.7235

Epoch 00074: loss did not improve from 0.07495
Epoch 75/100
2/2 [==============================] - 9s 4s/step - loss: 0.7811 - acc: 0.8330

Epoch 00075: loss did not improve from 0.07495
Epoch 76/100
2/2 [==============================] - 15s 7s/step - loss: 0.3334 - acc: 0.8906

Epoch 00076: loss did not improve from 0.07495
Epoch 77/100
2/2 [==============================] - 9s 4s/step - loss: 0.1911 - acc: 0.9424

Epoch 00077: loss did not improve from 0.07495
Epoch 78/100
2/2 [==============================] - 9s 4s/step - loss: 1.1763 - acc: 0.6851

Epoch 00078: loss did not improve from 0.07495
Epoch 79/100
2/2 [==============================] - 14s 7s/step - loss: 0.3865 - acc: 0.8750

Epoch 00079: loss did not improve from 0.07495
Epoch 80/100
2/2 [==============================] - 9s 4s/step - loss: 1.1487 - acc: 0.6343

Epoch 00080: loss did not improve from 0.07495
Epoch 81/100
2/2 [==============================] - 9s 4s/step - loss: 0.6123 - acc: 0.7822

Epoch 00081: loss did not improve from 0.07495
Epoch 82/100
2/2 [==============================] - 15s 7s/step - loss: 0.2101 - acc: 0.8906

Epoch 00082: loss did not improve from 0.07495
Epoch 83/100
2/2 [==============================] - 9s 4s/step - loss: 0.2292 - acc: 0.8589

Epoch 00083: loss did not improve from 0.07495
Epoch 84/100
2/2 [==============================] - 9s 4s/step - loss: 0.3671 - acc: 0.7946

Epoch 00084: loss did not improve from 0.07495
Epoch 85/100
2/2 [==============================] - 15s 7s/step - loss: 0.2955 - acc: 0.9375

Epoch 00085: loss did not improve from 0.07495
Epoch 86/100
2/2 [==============================] - 9s 4s/step - loss: 0.9231 - acc: 0.8330

Epoch 00086: loss did not improve from 0.07495
Epoch 87/100
2/2 [==============================] - 9s 4s/step - loss: 0.5067 - acc: 0.8522

Epoch 00087: loss did not improve from 0.07495
Epoch 88/100
2/2 [==============================] - 14s 7s/step - loss: 0.2957 - acc: 0.9062

Epoch 00088: loss did not improve from 0.07495
Epoch 89/100
2/2 [==============================] - 9s 4s/step - loss: 0.3855 - acc: 0.8973

Epoch 00089: loss did not improve from 0.07495
Epoch 90/100
2/2 [==============================] - 9s 4s/step - loss: 0.6497 - acc: 0.6084

Epoch 00090: loss did not improve from 0.07495
Epoch 91/100
2/2 [==============================] - 14s 7s/step - loss: 0.1241 - acc: 0.9531

Epoch 00091: loss did not improve from 0.07495
Epoch 92/100
2/2 [==============================] - 9s 4s/step - loss: 0.2602 - acc: 0.9165

Epoch 00092: loss did not improve from 0.07495
Epoch 93/100
2/2 [==============================] - 9s 4s/step - loss: 0.4100 - acc: 0.7495

Epoch 00093: loss did not improve from 0.07495
Epoch 94/100
2/2 [==============================] - 14s 7s/step - loss: 0.3437 - acc: 0.9219

Epoch 00094: loss did not improve from 0.07495
Epoch 95/100
2/2 [==============================] - 9s 4s/step - loss: 0.3342 - acc: 0.8973

Epoch 00095: loss did not improve from 0.07495
Epoch 96/100
2/2 [==============================] - 9s 4s/step - loss: 0.3566 - acc: 0.8138

Epoch 00096: loss did not improve from 0.07495
Epoch 97/100
2/2 [==============================] - 14s 7s/step - loss: 0.3389 - acc: 0.8906

Epoch 00097: loss did not improve from 0.07495
Epoch 98/100
2/2 [==============================] - 8s 4s/step - loss: 0.2129 - acc: 0.8973

Epoch 00098: loss did not improve from 0.07495
Epoch 99/100
2/2 [==============================] - 9s 4s/step - loss: 0.3721 - acc: 0.8465

Epoch 00099: loss did not improve from 0.07495
Epoch 100/100
2/2 [==============================] - 14s 7s/step - loss: 0.2046 - acc: 0.8906

Epoch 00100: loss did not improve from 0.07495
Serializing network...
Serializing label binarizer...